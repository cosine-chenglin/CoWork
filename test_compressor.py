#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å†å²åŠ¨ä½œå‹ç¼©åŠŸèƒ½
ç”Ÿæˆ200K tokensçš„å†…å®¹å¹¶æµ‹è¯•å‹ç¼©
"""

import sys
import random
import string
from services.action_compressor import ActionCompressor
from services.llm_client import SimpleLLMClient


def generate_random_text(token_count: int) -> str:
    """ç”ŸæˆæŒ‡å®štokenæ•°é‡çš„éšæœºæ–‡æœ¬"""
    # ä¸­æ–‡1.5å­—ç¬¦â‰ˆ1 tokenï¼Œè‹±æ–‡4å­—ç¬¦â‰ˆ1 token
    # æ··åˆç”Ÿæˆ
    chinese_chars = "çš„ä¸€æ˜¯åœ¨ä¸äº†æœ‰å’Œäººè¿™ä¸­å¤§ä¸ºä¸Šä¸ªå›½æˆ‘ä»¥è¦ä»–æ—¶æ¥ç”¨ä»¬ç”Ÿåˆ°ä½œåœ°äºå‡ºå°±åˆ†å¯¹æˆä¼šå¯ä¸»å‘å¹´åŠ¨åŒå·¥ä¹Ÿèƒ½ä¸‹è¿‡å­è¯´äº§ç§é¢è€Œæ–¹åå¤šå®šè¡Œå­¦æ³•æ‰€æ°‘å¾—ç»åä¸‰ä¹‹è¿›ç€ç­‰éƒ¨åº¦å®¶ç”µåŠ›é‡Œå¦‚æ°´åŒ–é«˜è‡ªäºŒç†èµ·å°ç‰©ç°å®åŠ é‡éƒ½ä¸¤ä½“åˆ¶æœºå½“ä½¿ç‚¹ä»ä¸šæœ¬å»æŠŠæ€§å¥½åº”å¼€å®ƒåˆè¿˜å› ç”±å…¶äº›ç„¶å‰å¤–å¤©æ”¿å››æ—¥é‚£ç¤¾ä¹‰äº‹å¹³å½¢ç›¸å…¨è¡¨é—´æ ·ä¸å…³å„é‡æ–°çº¿å†…æ•°æ­£å¿ƒåä½ æ˜çœ‹åŸåˆä¹ˆåˆ©æ¯”æˆ–ä½†è´¨æ°”ç¬¬å‘é“å‘½æ­¤å˜æ¡åªæ²¡ç»“è§£é—®æ„å»ºæœˆå…¬æ— ç³»å†›å¾ˆæƒ…è€…æœ€ç«‹ä»£æƒ³å·²é€šå¹¶æç›´é¢˜å…šç¨‹å±•äº”æœæ–™è±¡å‘˜é©ä½å…¥å¸¸æ–‡æ€»æ¬¡å“å¼æ´»è®¾åŠç®¡ç‰¹ä»¶é•¿æ±‚è€å¤´åŸºèµ„è¾¹æµè·¯çº§å°‘å›¾å±±ç»Ÿæ¥çŸ¥è¾ƒå°†ç»„è§è®¡åˆ«å¥¹æ‰‹è§’æœŸæ ¹è®ºè¿å†œæŒ‡å‡ ä¹åŒºå¼ºæ”¾å†³è¥¿è¢«å¹²åšå¿…æˆ˜å…ˆå›åˆ™ä»»å–æ®å¤„é˜Ÿå—ç»™è‰²å…‰é—¨å³ä¿æ²»åŒ—é€ ç™¾è§„çƒ­é¢†ä¸ƒæµ·å£ä¸œå¯¼å™¨å‹å¿—ä¸–é‡‘å¢äº‰æµé˜¶æ²¹æ€æœ¯æäº¤å—è”ä»€è®¤å…­å…±æƒæ”¶è¯æ”¹æ¸…å·±ç¾å†é‡‡è½¬æ›´å•é£åˆ‡æ‰“ç™½æ•™é€ŸèŠ±å¸¦å®‰åœºèº«è½¦ä¾‹çœŸåŠ¡å…·ä¸‡æ¯ç›®è‡³è¾¾èµ°ç§¯ç¤ºè®®å£°æŠ¥æ–—å®Œç±»å…«ç¦»ååç¡®æ‰ç§‘å¼ ä¿¡é©¬èŠ‚è¯ç±³æ•´ç©ºå…ƒå†µä»Šé›†æ¸©ä¼ åœŸè®¸æ­¥ç¾¤å¹¿çŸ³è®°éœ€æ®µç ”ç•Œæ‹‰æ—å¾‹å«ä¸”ç©¶è§‚è¶Šç»‡è£…å½±ç®—ä½æŒéŸ³ä¼—ä¹¦å¸ƒå¤å®¹å„¿é¡»é™…å•†ééªŒè¿æ–­æ·±éš¾è¿‘çŸ¿åƒå‘¨å§”ç´ æŠ€å¤‡åŠåŠé’çœåˆ—ä¹ å“çº¦æ”¯èˆ¬å²æ„ŸåŠ³ä¾¿å›¢å¾€é…¸å†å¸‚å…‹ä½•é™¤æ¶ˆæ„åºœç§°å¤ªå‡†ç²¾å€¼å·ç‡æ—ç»´åˆ’é€‰æ ‡å†™å­˜å€™æ¯›äº²å¿«æ•ˆæ–¯é™¢æŸ¥æ±Ÿå‹çœ¼ç‹æŒ‰æ ¼å…»æ˜“ç½®æ´¾å±‚ç‰‡å§‹å´ä¸“çŠ¶è‚²å‚äº¬è¯†é€‚å±åœ†åŒ…ç«ä½è°ƒæ»¡å¿å±€ç…§å‚çº¢ç»†å¼•å¬è¯¥é“ä»·ä¸¥"
    
    parts = []
    remaining_tokens = token_count
    
    while remaining_tokens > 0:
        # éšæœºé€‰æ‹©ç”Ÿæˆä¸­æ–‡æˆ–è‹±æ–‡
        if random.random() > 0.5:
            # ç”Ÿæˆä¸­æ–‡æ®µè½
            chunk_tokens = min(random.randint(50, 200), remaining_tokens)
            chunk_chars = int(chunk_tokens * 1.5)
            chunk = ''.join(random.choices(chinese_chars, k=chunk_chars))
            parts.append(chunk)
            remaining_tokens -= chunk_tokens
        else:
            # ç”Ÿæˆè‹±æ–‡æ®µè½
            chunk_tokens = min(random.randint(50, 200), remaining_tokens)
            chunk_chars = int(chunk_tokens * 4)
            chunk = ''.join(random.choices(string.ascii_letters + string.digits + ' ', k=chunk_chars))
            parts.append(chunk)
            remaining_tokens -= chunk_tokens
    
    return '\n\n'.join(parts)


def generate_large_action_history(target_tokens: int = 200000):
    """
    ç”Ÿæˆå¤§é‡çš„action_historyï¼Œç›®æ ‡è¾¾åˆ°200K tokens
    
    Args:
        target_tokens: ç›®æ ‡tokenæ•°
    
    Returns:
        List[Dict]: æ¨¡æ‹Ÿçš„actionå†å²
    """
    print(f"ğŸ”§ å¼€å§‹ç”Ÿæˆ {target_tokens} tokens çš„action_history...")
    
    action_history = []
    current_tokens = 0
    action_count = 0
    
    # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„å·¥å…·è°ƒç”¨
    tool_templates = [
        {
            "name": "file_read",
            "output_type": "æ–‡ä»¶å†…å®¹"
        },
        {
            "name": "web_search",
            "output_type": "æœç´¢ç»“æœ"
        },
        {
            "name": "code_execute",
            "output_type": "æ‰§è¡Œè¾“å‡º"
        },
        {
            "name": "document_parse",
            "output_type": "æ–‡æ¡£è§£æ"
        },
        {
            "name": "arxiv_search",
            "output_type": "è®ºæ–‡ä¿¡æ¯"
        }
    ]
    
    while current_tokens < target_tokens:
        action_count += 1
        template = random.choice(tool_templates)
        
        # æ¯ä¸ªactionç”Ÿæˆ5k-10k tokensçš„è¾“å‡º
        action_tokens = random.randint(5000, 10000)
        output_text = generate_random_text(action_tokens)
        
        action = {
            "tool_name": template["name"],
            "arguments": {
                "query": f"æµ‹è¯•æŸ¥è¯¢_{action_count}",
                "params": f"å‚æ•°_{action_count}"
            },
            "result": {
                "status": "success",
                "output": output_text
            }
        }
        
        action_history.append(action)
        current_tokens += action_tokens
        
        if action_count % 10 == 0:
            print(f"   å·²ç”Ÿæˆ {action_count} æ¡actionsï¼Œçº¦ {current_tokens} tokens")
    
    print(f"âœ… ç”Ÿæˆå®Œæˆï¼š{action_count} æ¡actionsï¼Œæ€»è®¡çº¦ {current_tokens} tokens\n")
    return action_history


def test_compression():
    """æµ‹è¯•å‹ç¼©åŠŸèƒ½"""
    print("="*80)
    print("ğŸ§ª å¼€å§‹æµ‹è¯•å†å²åŠ¨ä½œå‹ç¼©åŠŸèƒ½")
    print("="*80)
    print()
    
    # å¯ç”¨ LiteLLM è°ƒè¯•æ¨¡å¼
    try:
        import litellm
        litellm.set_verbose = True
        print("ğŸ› å·²å¯ç”¨ LiteLLM è°ƒè¯•æ¨¡å¼\n")
    except:
        pass
    
    # 1. åˆå§‹åŒ–LLMå®¢æˆ·ç«¯å’Œå‹ç¼©å™¨
    print("ğŸ“¦ åˆå§‹åŒ–LLMå®¢æˆ·ç«¯å’Œå‹ç¼©å™¨...")
    try:
        llm_client = SimpleLLMClient()
        compressor = ActionCompressor(llm_client)
        print(f"âœ… åˆå§‹åŒ–æˆåŠŸ")
        print(f"   æœ€å¤§ä¸Šä¸‹æ–‡çª—å£: {llm_client.max_context_window} tokens")
        print(f"   å‹ç¼©æ¨¡å‹: {llm_client.compressor_models[0]}")
        print()
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # 2. ç”Ÿæˆå¤§é‡æ•°æ®
    action_history = generate_large_action_history(target_tokens=200000)
    
    # 3. è®¡ç®—åŸå§‹tokenæ•°
    print("ğŸ“Š è®¡ç®—åŸå§‹tokenæ•°...")
    original_xml = compressor._actions_to_xml(action_history)
    original_tokens = compressor.count_tokens(original_xml)
    print(f"âœ… åŸå§‹æ•°æ®ç»Ÿè®¡:")
    print(f"   Actionsæ•°é‡: {len(action_history)} æ¡")
    print(f"   XMLé•¿åº¦: {len(original_xml)} å­—ç¬¦")
    print(f"   Tokenæ•°: {original_tokens} tokens")
    print()
    
    # 4. æµ‹è¯•å‹ç¼©
    print("ğŸ”„ å¼€å§‹å‹ç¼©æµ‹è¯•...")
    print(f"   è§¦å‘é˜ˆå€¼: {llm_client.max_context_window - 20000} tokens")
    print(f"   å½“å‰æ•°æ®: {original_tokens} tokens")
    print(f"   æ˜¯å¦éœ€è¦å‹ç¼©: {'âœ… æ˜¯' if original_tokens > llm_client.max_context_window - 20000 else 'âŒ å¦'}")
    print()
    
    # æ˜¾ç¤ºå°†è¦ä½¿ç”¨çš„å‹ç¼©æ¨¡å‹
    print("ğŸ¤– å‹ç¼©é…ç½®ä¿¡æ¯:")
    print(f"   å‹ç¼©æ¨¡å‹åˆ—è¡¨: {llm_client.compressor_models}")
    print(f"   å®é™…ä½¿ç”¨æ¨¡å‹: {llm_client.compressor_models[0]}")
    print(f"   LLMå®¢æˆ·ç«¯ç±»å‹: {type(llm_client).__name__}")
    print()
    
    # æ¨¡æ‹Ÿthinkingå’Œtask_input
    thinking = """
ä»»åŠ¡è¿›åº¦åˆ†æï¼š
1. âœ… å·²å®Œæˆæ•°æ®æ”¶é›†é˜¶æ®µ
2. âœ… å·²å®Œæˆåˆæ­¥åˆ†æ
3. ğŸ”„ æ­£åœ¨è¿›è¡Œæ·±åº¦åˆ†æ
4. â³ å¾…å®Œæˆç»“æœæ€»ç»“

ä¸‹ä¸€æ­¥è®¡åˆ’ï¼š
- ç»§ç»­åˆ†æå‰©ä½™æ•°æ®
- æå–å…³é”®ä¿¡æ¯
- ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
"""
    
    task_input = "æµ‹è¯•ä»»åŠ¡ï¼šåˆ†æå¤§é‡æ–‡æ¡£æ•°æ®å¹¶ç”Ÿæˆç»¼åˆæŠ¥å‘Š"
    
    try:
        # ä¸´æ—¶ä¿å­˜åŸå§‹çš„chatæ–¹æ³•æ¥ç›‘æ§è°ƒç”¨
        original_chat = llm_client.chat
        call_count = [0]  # ä½¿ç”¨åˆ—è¡¨æ¥åœ¨é—­åŒ…ä¸­ä¿®æ”¹
        
        def monitored_chat(*args, **kwargs):
            call_count[0] += 1
            model = kwargs.get('model', 'unknown')
            print(f"\nğŸ” LLMè°ƒç”¨ #{call_count[0]}:")
            print(f"   æ¨¡å‹: {model}")
            print(f"   å‚æ•°: tool_choice={kwargs.get('tool_choice', 'auto')}")
            if 'history' in kwargs and kwargs['history']:
                first_msg = kwargs['history'][0]
                content_preview = first_msg.content[:150] if hasattr(first_msg, 'content') else str(first_msg)[:150]
                print(f"   æ¶ˆæ¯é¢„è§ˆ: {content_preview}...")
            
            result = original_chat(*args, **kwargs)
            print(f"   è°ƒç”¨çŠ¶æ€: {result.status}")
            
            # å¦‚æœå¤±è´¥ï¼Œæ˜¾ç¤ºå®Œæ•´é”™è¯¯ä¿¡æ¯
            if result.status == "error":
                print(f"\n   âŒ å®Œæ•´é”™è¯¯ä¿¡æ¯:")
                print(f"   é”™è¯¯è¾“å‡º: {result.output}")
                if hasattr(result, 'error_information') and result.error_information:
                    print(f"\n   è¯¦ç»†é”™è¯¯å †æ ˆ:")
                    print("   " + "="*60)
                    # å°†é”™è¯¯ä¿¡æ¯æŒ‰è¡Œç¼©è¿›æ˜¾ç¤º
                    for line in str(result.error_information).split('\n'):
                        print(f"   {line}")
                    print("   " + "="*60)
            
            return result
        
        # æ›¿æ¢ä¸ºç›‘æ§ç‰ˆæœ¬
        llm_client.chat = monitored_chat
        
        compressed_history = compressor.compress_if_needed(
            action_history=action_history,
            max_context_window=llm_client.max_context_window,
            thinking=thinking,
            task_input=task_input
        )
        
        # æ¢å¤åŸå§‹æ–¹æ³•
        llm_client.chat = original_chat
        
        # 5. è®¡ç®—å‹ç¼©åçš„tokenæ•°
        print("\nğŸ“Š è®¡ç®—å‹ç¼©åtokenæ•°...")
        compressed_xml = compressor._actions_to_xml(compressed_history)
        compressed_tokens = compressor.count_tokens(compressed_xml)
        
        print(f"\n{'='*80}")
        print("âœ… å‹ç¼©æµ‹è¯•å®Œæˆ")
        print(f"{'='*80}")
        print(f"\nğŸ“ˆ å‹ç¼©æ•ˆæœå¯¹æ¯”:")
        print(f"   åŸå§‹Actions: {len(action_history)} æ¡")
        print(f"   å‹ç¼©åActions: {len(compressed_history)} æ¡")
        print(f"   Actionså‹ç¼©ç‡: {len(compressed_history)/len(action_history)*100:.1f}%")
        print()
        print(f"   åŸå§‹Tokens: {original_tokens:,} tokens")
        print(f"   å‹ç¼©åTokens: {compressed_tokens:,} tokens")
        print(f"   Tokenå‹ç¼©ç‡: {compressed_tokens/original_tokens*100:.1f}%")
        print(f"   èŠ‚çœTokens: {original_tokens - compressed_tokens:,} tokens")
        print()
        print(f"   åŸå§‹XMLé•¿åº¦: {len(original_xml):,} å­—ç¬¦")
        print(f"   å‹ç¼©åXMLé•¿åº¦: {len(compressed_xml):,} å­—ç¬¦")
        print()
        
        # 6. æ˜¾ç¤ºå‹ç¼©åçš„ç»“æ„
        print("ğŸ“‹ å‹ç¼©åçš„Actionsç»“æ„:")
        for i, action in enumerate(compressed_history):
            tool_name = action.get("tool_name", "unknown")
            result = action.get("result", {})
            output = result.get("output", "")
            is_summary = result.get("_is_summary", False)
            
            if is_summary:
                print(f"   [{i+1}] {tool_name} (å†å²æ€»ç»“)")
                print(f"       è¾“å‡ºé•¿åº¦: {len(output)} å­—ç¬¦")
                print(f"       è¾“å‡ºé¢„è§ˆ: {output[:100]}...")
            else:
                compressed_flag = result.get("_compressed", False)
                original_tokens_count = result.get("_original_tokens", 0)
                print(f"   [{i+1}] {tool_name} {'(å·²å‹ç¼©)' if compressed_flag else ''}")
                print(f"       å‚æ•°: {action.get('arguments', {})}")
                print(f"       è¾“å‡ºé•¿åº¦: {len(output)} å­—ç¬¦")
                if original_tokens_count:
                    print(f"       åŸå§‹tokens: {original_tokens_count}")
                print(f"       è¾“å‡ºé¢„è§ˆ: {output[:]}...")
            print()
        
        print(f"{'='*80}")
        print("âœ… æµ‹è¯•å®Œæˆï¼")
        print(f"{'='*80}")
        
    except Exception as e:
        print(f"\nâŒ å‹ç¼©è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_compression()

