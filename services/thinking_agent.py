#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Thinking Agent - ä»»åŠ¡è¿›å±•åˆ†ææœåŠ¡
"""

from typing import Dict, List
from services.llm_client import SimpleLLMClient, ChatMessage


class ThinkingAgent:
    """æ€è€ƒAgent - ç”¨äºåˆ†æä»»åŠ¡è¿›å±•"""
    
    def __init__(self):
        """åˆå§‹åŒ–Thinking Agent"""
        # ä½¿ç”¨ç®€åŒ–çš„LLMå®¢æˆ·ç«¯
        self.llm_client = SimpleLLMClient()
        
        # Thinking Agentçš„ç³»ç»Ÿæç¤ºè¯
        self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªä»»åŠ¡è¿›å±•åˆ†æä¸“å®¶ã€‚ä½ çš„èŒè´£æ˜¯ï¼š

1. åˆ†æå½“å‰ä»»åŠ¡çš„æ•´ä½“ç›®æ ‡
2. æ€»ç»“å·²å®Œæˆçš„å·¥ä½œ
3. è¯†åˆ«æ­£åœ¨è¿›è¡Œçš„ä»»åŠ¡
4. åˆ—å‡ºå‰©ä½™å¾…å®Œæˆçš„ä»»åŠ¡
5. è¯„ä¼°å½“å‰è¿›åº¦çŠ¶æ€

è¯·æä¾›æ¸…æ™°ã€ç»“æ„åŒ–çš„åˆ†æï¼ŒåŒ…æ‹¬ï¼š
- ä»»åŠ¡æ¦‚è¿°
- å·²å®Œæˆé¡¹ç›®
- å½“å‰çŠ¶æ€
- å‰©ä½™ä»»åŠ¡
- ä¸‹ä¸€æ­¥è¡ŒåŠ¨
- é£é™©è¯„ä¼°ï¼ˆå¦‚æœ‰ï¼‰

**é‡è¦**ï¼š
- å¦‚æœå‘ç°Agenté™·å…¥æ­»å¾ªç¯ï¼Œç”¨ä¸¥å‰è¯­æ°”è­¦å‘Šå¹¶æ˜ç¡®ä¸‹ä¸€æ­¥ä»»åŠ¡
- å¦‚æœå‘ç°Agentæ‰§è¡Œäº†èŒè´£å¤–çš„å·¥ä½œï¼Œç«‹å³è­¦å‘Šåœæ­¢
- è¿›åº¦å¿…é¡»ç²¾å‡†ï¼Œä¾‹å¦‚codingä»»åŠ¡è¦æ˜ç¡®å·²å®ç°å¤šå°‘åŠŸèƒ½ã€å®Œæˆå¤šå°‘æ–‡ä»¶
- ä½¿ç”¨ä¸­æ–‡è¾“å‡º
"""
    
    def analyze_first_thinking(self, task_description: str, agent_system_prompt: str, 
                               available_tools: List[str]) -> str:
        """
        é¦–æ¬¡æ€è€ƒ - åˆå§‹è§„åˆ’
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            agent_system_prompt: Agentçš„ç³»ç»Ÿæç¤ºè¯
            available_tools: å¯ç”¨å·¥å…·åˆ—è¡¨
            
        Returns:
            åˆå§‹è§„åˆ’ç»“æœ
        """
        try:
            # æ„å»ºåˆ†æè¯·æ±‚
            analysis_request = f"""å½“å‰ä»»åŠ¡ï¼š{task_description}

Agentçš„ç³»ç»Ÿæç¤ºè¯å’Œå·¥ä½œæµç¨‹ï¼š
{agent_system_prompt}

å¯ç”¨å·¥å…·ï¼š{', '.join(available_tools)}

è¿™æ˜¯ä»»åŠ¡çš„åˆå§‹é˜¶æ®µï¼Œè¯·è¿›è¡Œåˆå§‹è§„åˆ’ï¼š
1. ç†è§£ä»»åŠ¡ç›®æ ‡
2. è§„åˆ’æ‰§è¡Œæ­¥éª¤
3. ç¡®å®šéœ€è¦ä½¿ç”¨çš„å·¥å…·
4. é¢„åˆ¤å¯èƒ½çš„é£é™©

è¯·æä¾›ç®€æ´ä½†å…¨é¢çš„åˆå§‹è§„åˆ’ã€‚"""
            
            history = [ChatMessage(role="user", content=analysis_request)]
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹ï¼Œä¸éœ€è¦å·¥å…·
            response = self.llm_client.chat(
                history=history,
                model=self.llm_client.models[0],
                system_prompt=self.system_prompt,
                tool_list=[],  # Thinkingä¸ä½¿ç”¨å·¥å…·
                tool_choice="auto"
            )
            
            if response.status == "success":
                return f"[ğŸ¤– åˆå§‹è§„åˆ’]\n\n{response.output}"
            else:
                return f"[åˆå§‹è§„åˆ’å¤±è´¥: {response.error_information}]"
        
        except Exception as e:
            print(f"âš ï¸ é¦–æ¬¡thinkingå¤±è´¥: {e}")
            return f"[åˆå§‹è§„åˆ’å¤±è´¥: {str(e)}]"
    
    def analyze_progress(self, task_description: str, agent_system_prompt: str,
                        tool_call_counter: int) -> str:
        """
        è¿›åº¦åˆ†æ - å‘¨æœŸæ€§åˆ†æ
        
        Args:
            task_description: ä»»åŠ¡æè¿°
            agent_system_prompt: Agentçš„å®Œæ•´ç³»ç»Ÿæç¤ºè¯ï¼ˆåŒ…å«<å†å²åŠ¨ä½œ>ï¼‰
            tool_call_counter: å·¥å…·è°ƒç”¨è®¡æ•°
            
        Returns:
            è¿›åº¦åˆ†æç»“æœ
        """
        try:
            # æ„å»ºåˆ†æè¯·æ±‚ï¼ˆagent_system_promptå·²åŒ…å«å®Œæ•´çš„<å†å²åŠ¨ä½œ>ï¼‰
            analysis_request = f"""å½“å‰ä»»åŠ¡ï¼š{task_description}

Agentçš„å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«ç³»ç»Ÿè§’è‰²ã€å†å²åŠ¨ä½œç­‰ï¼‰ï¼š
{agent_system_prompt}

å·²æ‰§è¡Œçš„å·¥å…·è°ƒç”¨æ•°ï¼š{tool_call_counter}

åŸºäºä»¥ä¸Šå®Œæ•´ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯·åˆ†æï¼š
1. ä»»åŠ¡è¿›å±•åˆ°ä»€ä¹ˆç¨‹åº¦ï¼Ÿ
2. å·²å®Œæˆå“ªäº›ä»»åŠ¡ï¼Ÿ
3. è¿˜éœ€è¦å®Œæˆä»€ä¹ˆï¼Ÿ
4. å½“å‰æ‰§è¡ŒçŠ¶æ€å¦‚ä½•ï¼Ÿ
5. Agentæ˜¯å¦æ­£ç¡®éµå¾ªå…¶ç³»ç»Ÿæç¤ºè¯ï¼Ÿ
6. ä¸‹ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆï¼Ÿï¼ˆåªå»ºè®®å½“å‰Agentçš„ä¸‹ä¸€æ­¥ï¼ï¼‰
7. æ˜¯å¦æœ‰é—æ¼çš„æ­¥éª¤æˆ–æ³¨æ„äº‹é¡¹ï¼Ÿ
8. åˆ—å‡ºAgentæœªæ¥å¯èƒ½ä½¿ç”¨çš„æ‰€æœ‰æ–‡ä»¶è·¯å¾„å’Œæè¿°

**å…³é”®**ï¼š
- è¿›åº¦å¿…é¡»ç²¾å‡†ï¼
- å¦‚æœå‘ç°æ­»å¾ªç¯ï¼Œä¸¥å‰è­¦å‘Š
- å¦‚æœå‘ç°è¶Šç•Œæ“ä½œï¼Œç«‹å³æŒ‡å‡º"""
            
            history = [ChatMessage(role="user", content=analysis_request)]
            
            response = self.llm_client.chat(
                history=history,
                model=self.llm_client.models[0],
                system_prompt=self.system_prompt,
                tool_list=[],
                tool_choice="auto"
            )
            
            if response.status == "success":
                return f"[ğŸ¤– è¿›åº¦åˆ†æ - ç¬¬{tool_call_counter}è½®]\n\n{response.output}"
            else:
                return f"[è¿›åº¦åˆ†æå¤±è´¥: {response.error_information}]"
        
        except Exception as e:
            print(f"âš ï¸ è¿›åº¦åˆ†æå¤±è´¥: {e}")
            return f"[è¿›åº¦åˆ†æå¤±è´¥: {str(e)}]"


if __name__ == "__main__":
    # æµ‹è¯•Thinking Agent
    thinking_agent = ThinkingAgent()
    
    result = thinking_agent.analyze_first_thinking(
        task_description="ç”Ÿæˆæ–æ³¢é‚£å¥‘æ•°åˆ—æ–‡ä»¶",
        agent_system_prompt="ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹åŠ©æ‰‹",
        available_tools=["file_write", "execute_code"]
    )
    
    print("="*80)
    print(result)
    print("="*80)

