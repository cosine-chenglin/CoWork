#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„LLMå®¢æˆ·ç«¯ - ä½¿ç”¨LiteLLMç»Ÿä¸€æ¥å£
"""

import os
import yaml
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from pathlib import Path
from litellm import completion  # ç›´æ¥å¯¼å…¥completionå‡½æ•°
import litellm


@dataclass
class ChatMessage:
    """èŠå¤©æ¶ˆæ¯"""
    role: str
    content: str


@dataclass
class ToolCall:
    """å·¥å…·è°ƒç”¨"""
    id: str
    name: str
    arguments: Dict[str, Any]


@dataclass
class LLMResponse:
    """LLMå“åº”"""
    status: str  # "success" or "error"
    output: str
    tool_calls: List[ToolCall]
    model: str
    finish_reason: str
    usage: Optional[Dict] = None
    error_information: str = ""


class SimpleLLMClient:
    """ç®€åŒ–çš„LLMå®¢æˆ·ç«¯ - åŸºäºLiteLLM"""
    
    def __init__(self, llm_config_path: str = None, tools_config_path: str = None):
        """
        åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        
        Args:
            llm_config_path: LLMé…ç½®æ–‡ä»¶è·¯å¾„
            tools_config_path: å·¥å…·é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½LLMé…ç½®
        if llm_config_path is None:
            project_root = Path(__file__).parent.parent
            llm_config_path = project_root / "config" / "run_env_config" / "llm_config.yaml"
        
        if not os.path.exists(llm_config_path):
            raise FileNotFoundError(f"LLMé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {llm_config_path}")
        
        with open(llm_config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # è¯»å–é…ç½®
        self.base_url = self.config.get("base_url", "")
        self.api_key = self.config.get("api_key", "")
        self.models = self.config.get("models", [])
        self.temperature = self.config.get("temperature", 0)
        self.max_tokens = self.config.get("max_tokens", 0)
        self.max_context_window = self.config.get("max_context_window", 100000)  # ä¸Šä¸‹æ–‡çª—å£é™åˆ¶
        
        if not self.api_key:
            raise ValueError("æœªé…ç½®APIå¯†é’¥")
        
        if not self.models:
            raise ValueError("æœªé…ç½®å¯ç”¨æ¨¡å‹åˆ—è¡¨")
        
        # åŠ è½½å·¥å…·é…ç½®
        self.tools_config = {}
        if tools_config_path and os.path.exists(tools_config_path):
            with open(tools_config_path, 'r', encoding='utf-8') as f:
                self.tools_config = yaml.safe_load(f)
        
        # é…ç½®LiteLLM
        litellm.set_verbose = False  # å…³é—­è¯¦ç»†æ—¥å¿—
        
        print(f"âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ˆLiteLLMï¼‰")
        print(f"   Base URL: {self.base_url}")
        print(f"   å¯ç”¨æ¨¡å‹: {len(self.models)} ä¸ª")
        print(f"   é»˜è®¤Temperature: {self.temperature}")
        print(f"   é»˜è®¤Max Tokens: {self.max_tokens}")
    
    def chat(
        self,
        history: List[ChatMessage],
        model: str,
        system_prompt: str,
        tool_list: List[str],
        tool_choice: str = "required",
        temperature: float = None,
        max_tokens: int = None
    ) -> LLMResponse:
        """
        è°ƒç”¨LLMè¿›è¡Œå¯¹è¯
        
        Args:
            history: å¯¹è¯å†å²
            model: æ¨¡å‹åç§°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            tool_list: å¯ç”¨å·¥å…·åˆ—è¡¨
            tool_choice: å·¥å…·é€‰æ‹©ç­–ç•¥
            temperature: æ¸©åº¦å‚æ•°ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶é»˜è®¤å€¼ï¼‰
            max_tokens: æœ€å¤§tokenæ•°ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶é»˜è®¤å€¼ï¼‰
            
        Returns:
            LLMResponseå¯¹è±¡
        """
        # ä½¿ç”¨é…ç½®æ–‡ä»¶çš„é»˜è®¤å€¼
        if temperature is None:
            temperature = self.temperature
        if max_tokens is None:
            max_tokens = self.max_tokens
        
        try:
            # æ„å»ºå·¥å…·å®šä¹‰ï¼ˆOpenAIæ ¼å¼ï¼‰
            tools_definition = self._build_tools_definition(tool_list)
            
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            messages = [{"role": "system", "content": system_prompt}]
            messages.extend([{"role": msg.role, "content": msg.content} for msg in history])
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            kwargs = {
                "model": model,
                "messages": messages,
                "temperature": temperature,
                "api_key": self.api_key,
                "api_base": self.base_url  # æ³¨æ„ï¼šLiteLLMä½¿ç”¨api_baseè€Œä¸æ˜¯base_url
            }
            
            # åªåœ¨max_tokens > 0æ—¶æ·»åŠ 
            if max_tokens > 0:
                kwargs["max_tokens"] = max_tokens
            
            # æ·»åŠ å·¥å…·å®šä¹‰
            if tools_definition:
                kwargs["tools"] = tools_definition
                if tool_choice == "required":
                    kwargs["tool_choice"] = "required"
                # ç¦ç”¨å¹¶è¡Œå·¥å…·è°ƒç”¨ï¼ˆæ¯æ¬¡åªè°ƒç”¨ä¸€ä¸ªå·¥å…·ï¼‰
                kwargs["parallel_tool_calls"] = False
            
            # ä½¿ç”¨LiteLLMè°ƒç”¨
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            print(f"   ğŸ“ System Prompté•¿åº¦: {len(system_prompt)} å­—ç¬¦")
            print(f"   ğŸ”§ å·¥å…·æ•°é‡: {len(tools_definition)}")
            print(f"   ğŸ“¨ æ¶ˆæ¯æ•°é‡: {len(messages)}")
            
            response = completion(**kwargs)  # ä½¿ç”¨å¯¼å…¥çš„å‡½æ•°
            
            # è§£æå“åº”ï¼ˆå‚è€ƒåŸé¡¹ç›®çš„å®‰å…¨è§£ææ–¹å¼ï¼‰
            if response.choices and len(response.choices) > 0:
                choice = response.choices[0]
                message = choice.message
                
                output_text = message.content or ""
                tool_calls = []
                
                # å®‰å…¨è§£æå·¥å…·è°ƒç”¨
                if hasattr(message, 'tool_calls') and message.tool_calls:
                    for tc in message.tool_calls:
                        import json
                        # å®‰å…¨è§£æå‚æ•°
                        try:
                            if isinstance(tc.function.arguments, str):
                                arguments = json.loads(tc.function.arguments)
                            else:
                                arguments = tc.function.arguments
                        except:
                            arguments = {}
                        
                        tool_calls.append(ToolCall(
                            id=tc.id,
                            name=tc.function.name,
                            arguments=arguments
                        ))
                
                # å®‰å…¨æå–usageä¿¡æ¯
                usage = None
                if hasattr(response, 'usage') and response.usage:
                    usage = {
                        "prompt_tokens": getattr(response.usage, 'prompt_tokens', 0),
                        "completion_tokens": getattr(response.usage, 'completion_tokens', 0),
                        "total_tokens": getattr(response.usage, 'total_tokens', 0)
                    }
            else:
                return LLMResponse(
                    status="error",
                    output="",
                    tool_calls=[],
                    model=model,
                    finish_reason="error",
                    error_information="å“åº”æ ¼å¼å¼‚å¸¸ï¼šç¼ºå°‘choiceså­—æ®µ"
                )
            
            return LLMResponse(
                status="success",
                output=output_text,
                tool_calls=tool_calls,
                model=response.model,
                finish_reason=response.choices[0].finish_reason,
                usage=usage
            )
        
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            return LLMResponse(
                status="error",
                output="",
                tool_calls=[],
                model=model,
                finish_reason="error",
                error_information=f"{str(e)}\n\nDetails:\n{error_detail}"
            )
    
    def set_tools_config(self, tools_config: Dict):
        """
        è®¾ç½®å·¥å…·é…ç½®ï¼ˆä»ConfigLoaderä¼ å…¥ï¼‰
        
        Args:
            tools_config: å·¥å…·é…ç½®å­—å…¸
        """
        self.tools_config = tools_config
    
    def _build_tools_definition(self, tool_list: List[str]) -> List[Dict]:
        """æ„å»ºå·¥å…·å®šä¹‰ï¼ˆOpenAIæ ¼å¼ï¼‰"""
        if not self.tools_config:
            return []
        
        tools = []
        for tool_name in tool_list:
            if tool_name in self.tools_config:
                tool_config = self.tools_config[tool_name]
                tools.append({
                    "type": "function",
                    "function": {
                        "name": tool_config.get("name", tool_name),
                        "description": tool_config.get("description", ""),
                        "parameters": tool_config.get("parameters", {})
                    }
                })
        
        return tools


if __name__ == "__main__":
    # æµ‹è¯•LLMå®¢æˆ·ç«¯
    try:
        client = SimpleLLMClient()
        print(f"âœ… å¯ç”¨æ¨¡å‹: {client.models}")
        
        # æµ‹è¯•ç®€å•è°ƒç”¨
        history = [ChatMessage(role="user", content="è¯·è¾“å‡ºä¸‹ä¸€ä¸ªåŠ¨ä½œ")]
        response = client.chat(
            history=history,
            model=client.models[0],  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨æ¨¡å‹
            system_prompt="ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œè¯·ä½¿ç”¨å·¥å…·æ¥å®Œæˆä»»åŠ¡ã€‚",
            tool_list=["file_read", "file_write"],
            tool_choice="required"
        )
        
        print(f"âœ… å“åº”çŠ¶æ€: {response.status}")
        print(f"âœ… å·¥å…·è°ƒç”¨æ•°é‡: {len(response.tool_calls)}")
        if response.tool_calls:
            print(f"âœ… ç¬¬ä¸€ä¸ªå·¥å…·: {response.tool_calls[0].name}")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
